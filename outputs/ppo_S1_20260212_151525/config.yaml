agent:
  action_type: continuous
  max_depth: 50.0
  max_velocity: 1.5
  min_depth: 2.0
  n_concentration_samples: 8
  n_discrete_actions: 8
  sensor_radius: 50
  spawn_margin: 100
  spawn_mode: near_shore
data:
  concentration_var: Concentration - component 1
  fill_value: 0.0
  interpolation: linear
  nc_pattern: CMEMS_*_conc_grid_10m.nc
  time_var: time
  x_var: x
  y_var: y
domain:
  grid_resolution: 10
  nx: 300
  ny: 250
  xmax: 622000
  xmin: 619000
  ymax: 4797000
  ymin: 4794500
environment:
  dt: 10
  max_episode_steps: 500
  normalize_obs: true
  normalize_reward: false
  reward:
    boundary_penalty: -10.0
    concentration_gradient_scale: 10.0
    distance_threshold: 50
    source_reached_bonus: 100.0
    source_threshold: 0.8
    step_penalty: -0.1
multi_agent:
  communication_range: 100
  coordination_bonus: 5.0
  enabled: false
  n_agents: 3
  shared_reward: false
sources:
  S1:
    x: 620100
    y: 4796210
  S2:
    x: 619800
    y: 4795900
  S3:
    x: 620200
    y: 4795800
  concentration: 1000
  flow_rate: 50
synthetic:
  enabled: true
  gaussian:
    advection_velocity:
    - 0.1
    - 0.05
    diffusion_coef: 5.0
    source_strength: 1000
  plume_model: gaussian
  time_varying: true
  wind_variability: 0.2
training:
  algorithm: PPO
  batch_size: 64
  clip_range: 0.2
  ent_coef: 0.05
  eval_freq: 10000
  gae_lambda: 0.95
  gamma: 0.99
  learning_rate: 0.0003
  max_grad_norm: 0.5
  n_epochs: 10
  n_eval_episodes: 10
  n_steps: 2048
  policy: MlpPolicy
  policy_kwargs:
    activation_fn: tanh
    net_arch:
      pi:
      - 256
      - 256
      vf:
      - 256
      - 256
  save_freq: 50000
  tensorboard_log: ./logs/tensorboard
  total_timesteps: 1000000
  verbose: 1
  vf_coef: 0.5
