# HYDRAS Source Seeking

**Addestramento di agenti RL per la localizzazione di sorgenti di inquinante marino**

Basato sulle simulazioni idrodinamiche MIKE21 per l'area del Porto di Cecina, questo progetto implementa un sistema di Reinforcement Learning per addestrare agenti (AUV - Autonomous Underwater Vehicles) a localizzare sorgenti di inquinanti in ambiente marino.

## ğŸ¯ Obiettivo

Addestrare una rete di agenti autonomi capaci di:
1. Navigare in un campo di concentrazione di inquinante
2. Seguire il gradiente di concentrazione
3. Localizzare la sorgente di emissione

## ğŸ“ Struttura del Progetto

```
hydras_source_seeking/
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ config.yaml          # Configurazione principale
â”œâ”€â”€ envs/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ source_seeking_env.py # Ambiente Gymnasium
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ data_loader.py       # Loader dati NetCDF + generatore sintetico
â”œâ”€â”€ agents/                   # (per estensioni future)
â”œâ”€â”€ data/                     # Directory per file .nc
â”œâ”€â”€ train_ppo.py             # Script di training PPO
â”œâ”€â”€ visualize.py             # Tools di visualizzazione
â”œâ”€â”€ requirements.txt         # Dipendenze
â””â”€â”€ README.md
```

## ğŸš€ Quick Start

### 1. Installazione

```bash
# Clona o copia il progetto
cd hydras_source_seeking

# Crea ambiente virtuale (consigliato)
python -m venv venv
source venv/bin/activate  # Linux/Mac
# oppure: venv\Scripts\activate  # Windows

# Installa dipendenze
pip install -r requirements.txt
```

### 2. Training con dati sintetici

```bash
# Training base (usa dati sintetici)
python train_ppo.py train --source S1 --n-envs 4 --timesteps 500000

# Training con piÃ¹ ambienti paralleli (piÃ¹ veloce)
python train_ppo.py train --source S1 --n-envs 8 --timesteps 1000000
```

### 3. Training con dati NetCDF

```bash
# Copia i file .nc nella cartella data/
cp /path/to/CMEMS_S1_01_conc_grid_10m.nc data/

# Training con dati reali
python train_ppo.py train --source S1 --nc-file data/CMEMS_S1_01_conc_grid_10m.nc
```

### 4. Valutazione

```bash
# Valuta il modello addestrato
python train_ppo.py eval outputs/ppo_S1_*/models/final_model.zip --episodes 20

# Con rendering
python train_ppo.py eval outputs/ppo_S1_*/models/best/best_model.zip --render
```

## âš™ï¸ Configurazione

Il file `configs/config.yaml` contiene tutti i parametri configurabili:

### Dominio
```yaml
domain:
  xmin: 619000    # Coordinate UTM
  xmax: 622000
  ymin: 4794500
  ymax: 4797000
  grid_resolution: 10  # metri
```

### Agente
```yaml
agent:
  max_velocity: 1.5      # m/s
  sensor_radius: 50      # m
  n_concentration_samples: 8
  action_type: "continuous"  # o "discrete"
```

### Reward
```yaml
environment:
  reward:
    source_reached_bonus: 100.0
    concentration_gradient_scale: 10.0
    step_penalty: -0.1
    boundary_penalty: -10.0
    distance_threshold: 30  # metri per "successo"
```

### PPO Hyperparameters
```yaml
training:
  learning_rate: 0.0003
  n_steps: 2048
  batch_size: 64
  n_epochs: 10
  gamma: 0.99
  gae_lambda: 0.95
  clip_range: 0.2
```

## ğŸ”¬ Dettagli Tecnici

### Observation Space (15 dimensioni)
- **8 concentrazioni** campionate in cerchio attorno all'agente (raggio 50m)
- **1 concentrazione** al centro (posizione agente)
- **2 componenti** del gradiente normalizzato
- **2 coordinate** posizione normalizzata [-1, 1]
- **2 componenti** velocitÃ  normalizzata

### Action Space
- **Continuous**: `[vx, vy]` in [-1, 1], scalato a Â±1.5 m/s
- **Discrete**: 8 direzioni + stazionario (9 azioni)

### Reward Shaping
1. **+100** per raggiungere la sorgente (< 30m)
2. **+10 Ã— alignment** per allineamento con gradiente
3. **+1 Ã— Î”concentrazione** per aumento concentrazione
4. **+0.1 Ã— Î”distanza** per avvicinamento
5. **-0.1** penalitÃ  per step (incentiva velocitÃ )
6. **-10** per uscita dal dominio

## ğŸ“Š Visualizzazione

```python
from visualize import plot_training_summary, create_animation
from utils.data_loader import DataManager

# Carica dati
dm = DataManager(use_synthetic=True)
field = dm.get_concentration_field(source_id='S1')

# Visualizza campo
from visualize import plot_concentration_field
import matplotlib.pyplot as plt

fig, ax = plt.subplots(figsize=(12, 10))
plot_concentration_field(field, ax=ax)
plt.savefig('concentration_field.png')

# Dopo training, visualizza traiettorie
trajectory = np.load('trajectory.npy')  # salvata durante eval
plot_training_summary(trajectory, field, save_path='summary.png')
```

## ğŸ”„ Estensione Multi-Agente

Il sistema Ã¨ progettato per essere esteso a scenari multi-agente. Vedi `configs/config.yaml`:

```yaml
multi_agent:
  enabled: false  # Abilitare per multi-agente
  n_agents: 3
  communication_range: 100  # metri
  shared_reward: false
  coordination_bonus: 5.0
```

Per l'implementazione multi-agente, si consiglia:
- **PettingZoo** per ambienti multi-agente
- **MAPPO** (Multi-Agent PPO) per training coordinato
- **Communication protocols** per condivisione informazioni tra agenti

## ğŸ“ˆ Monitoraggio Training

```bash
# Avvia TensorBoard
tensorboard --logdir outputs/ppo_S1_*/logs/tensorboard

# Apri browser: http://localhost:6006
```

Metriche monitorate:
- `rollout/ep_rew_mean`: Reward medio per episodio
- `rollout/ep_len_mean`: Lunghezza media episodi
- `custom/success_rate`: Tasso di successo
- `custom/avg_final_distance`: Distanza media finale dalla sorgente

## ğŸ—‚ï¸ Dati delle Simulazioni DICEA

Le simulazioni provengono da MIKE21 con:
- **Dominio**: ~16Ã—14 km attorno al Porto di Cecina
- **Risoluzione**: 10m nella zona di interesse
- **Sorgenti**: S1, S2, S3 con portata 50 l/s e concentrazione 1000 g/mÂ³
- **Forzanti**: Dati CMEMS (correnti) + vento misurato
- **Output**: NetCDF con passo 10m e intervallo 1 minuto

## ğŸ“ Note

- Il generatore sintetico usa un modello advection-diffusion semplificato
- Per risultati realistici, usare i file NetCDF delle simulazioni MIKE21
- Il training richiede circa 500k-1M timesteps per convergenza
- Consigliato: GPU per training piÃ¹ veloce (PPO supporta CUDA)

## ğŸ“š Riferimenti

- **MIKE21**: DHI Flow Model FM
- **CMEMS**: Copernicus Marine Environment Monitoring Service
- **PPO**: Schulman et al., "Proximal Policy Optimization Algorithms" (2017)
- **Stable-Baselines3**: https://stable-baselines3.readthedocs.io/

## ğŸ‘¥ Autori

- Progetto HYDRAS
- Simulazioni: DICEA
- Implementazione RL: [Il tuo nome]

## ğŸ“„ Licenza

[Da definire]
